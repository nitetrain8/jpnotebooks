{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "url_tmpl = \"https://issue.pbsbiotech.com/projects/%s/issues.csv?utf8=%%E2%%9C%%93&columns=all\"\n",
    "_p_urls = [\n",
    "    \"pbscustomer\", \"pbsdisposables\", \"pbsinstruments\", \n",
    "    \"magic-metals\", \"manufacturing\", \"pbssoftware\", \"swtesting\",\n",
    "    \"system-qualification-testing\"\n",
    "]\n",
    "project_urls = [url_tmpl % p for p in _p_urls]\n",
    "project_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 661,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\" Issuetracker API \n",
    "\n",
    "* TODO: Create IssueList class (?)\n",
    "* Parse Gantt HTML for class 'issue-subject' using style:width to determine hierarchy\n",
    "* Consider method of lazy evaluation of issue field generation by\n",
    "calling back to API to download project issues CSV, and update all issues\n",
    "in project. \n",
    "* Implement issue caching\n",
    "\n",
    "Issue():\n",
    "    * Add programmatic logging of all fields seen, ever.\n",
    "    * Map fields seen to types and conversion functions\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import requests\n",
    "import urllib\n",
    "import pyquery\n",
    "from collections import OrderedDict\n",
    "import re\n",
    "import dateutil.parser\n",
    "import lxml\n",
    "\n",
    "uj = urllib.parse.urljoin\n",
    "_sp_re = re.compile(r\"(\\d*?) (subproject)?(s{0,1})\")\n",
    "_name2id_re = re.compile(r\"(.*?)\\s*?#(\\d*)$\")\n",
    "\n",
    "class IssuetrackerAPI():\n",
    "    _login_url = \"/login\"\n",
    "    _proj_issues_url = \"/projects/%s/issues\"\n",
    "    _issues_url = \"/issues\"\n",
    "    _proj_url = \"/projects\"\n",
    "    \n",
    "    def __init__(self, base_url, username=None, pw=None):\n",
    "        r = urllib.parse.urlparse(base_url)\n",
    "        if not r.scheme and not r.netloc:\n",
    "            base_url = urllib.parse.urlunparse((\"https\", r.path, \"\", r.params, r.query, r.fragment))\n",
    "        self._base_url = base_url\n",
    "        self._sess = requests.Session()\n",
    "        \n",
    "        if username is None or pw is None:\n",
    "            raise ValueError(\"Must have valid username and password.\")\n",
    "        \n",
    "        self._username = username\n",
    "        self._password = pw\n",
    "        self._auth = (username, pw)\n",
    "        \n",
    "        self._cache = {}\n",
    "        \n",
    "    def copy(self):\n",
    "        cls = self.__class__\n",
    "        new = cls(self._base_url, self._username, self._password)\n",
    "        return new\n",
    "    \n",
    "    def issues(self):\n",
    "        raise NotImplemented\n",
    "        \n",
    "    def projects(self, project_id=None):\n",
    "        \"\"\" Return dict of projects if project_id is None, or project matching \n",
    "        `project_id`.\n",
    "        \"\"\"\n",
    "        pj = self._cache.get(\"projects\")\n",
    "        if not pj:\n",
    "            pj = self.download_projects()\n",
    "            self._cache['projects'] = pj\n",
    "        \n",
    "        if project_id is not None:\n",
    "            for attr in (\"id\", \"name\", \"identifier\"):\n",
    "                for p in pj.values():\n",
    "                    if getattr(p, attr) == project_id:\n",
    "                        return p\n",
    "        return pj\n",
    "            \n",
    "    def login(self):\n",
    "        r1 = self._sess.get(self._base_url)\n",
    "        r1.raise_for_status()\n",
    "        q = pyquery.PyQuery(r1.content)\n",
    "        data = {}\n",
    "        for td in q(\"#login-form :input\"):\n",
    "            at = td.attrib\n",
    "            if 'name' in at:\n",
    "                k = at['name']\n",
    "                v = at.get('value', \"\")\n",
    "                data[k] = v\n",
    "                \n",
    "        data['username'] = self._username\n",
    "        data['password'] = self._password\n",
    "        \n",
    "        body = urllib.parse.urlencode(data)\n",
    "        r2 = self._sess.post(uj(self._base_url, self._login_url), body)\n",
    "        r2.raise_for_status()\n",
    "        if not pyquery.PyQuery(r2.content)(\"#loggedas\"):\n",
    "            raise ValueError(\"Invalid Username or Password\")\n",
    "        return r2\n",
    "        \n",
    "    def download_project_issues_csv(self, project, utf8=True, columns='all'):\n",
    "        r = self._download_project_csv(project, utf8, columns)\n",
    "        return self._parse_proj_csv(r.content)\n",
    "    \n",
    "    def _download_project_csv(self, project, utf8, columns):\n",
    "        if utf8:\n",
    "            utf8 = \"%E2%9C%93\"\n",
    "        else:\n",
    "            utf8 = \"\"\n",
    "        url_end = \".csv?utf8=%s&columns=%s\" \n",
    "        url = (self._proj_issues_url + url_end) % (project, utf8, columns)\n",
    "        url = uj(self._base_url, url)\n",
    "        r = self._sess.get(url)\n",
    "        r.raise_for_status()\n",
    "        return r\n",
    "    \n",
    "    def download_issue_pdf(self, id):\n",
    "        href = self._issues_url + \"/\" + str(id)\n",
    "        return self.download_issue_pdf2(href)\n",
    "\n",
    "    def download_issue_pdf2(self, href):\n",
    "        \"\"\" Sometimes it is more convenient to access issue by provided \n",
    "        href. \"\"\"\n",
    "        type = \".pdf\"\n",
    "        url = uj(self._base_url, href + type)\n",
    "        r = self._sess.get(url)\n",
    "        r.raise_for_status()\n",
    "        return r.content\n",
    "        \n",
    "    def _parse_proj_csv(self, csv, encoding='utf-8'):\n",
    "        if not isinstance(csv, str):\n",
    "            csv = csv.decode(encoding)\n",
    "        sl = csv.splitlines()\n",
    "        sl[0] = sl[0].lower().replace('\"', \"\")\n",
    "        lines = [l.split(\",\") for l in sl]\n",
    "        issues = OrderedDict()\n",
    "        for i, l in enumerate(lines[1:], 1):\n",
    "            issue = _Issue(line=sl[i], api=self)\n",
    "            for key, val in zip(lines[0], l):\n",
    "                issue[key] = val.strip('\"') or \"<n/a>\"\n",
    "            issue['#'] = int(issue['#'])\n",
    "            issues[issue['#']] = issue\n",
    "        return issues\n",
    "    \n",
    "#     def download_projects(self):\n",
    "#         url = uj(self._base_url, self._proj_url)\n",
    "#         r = self._sess.get(url)\n",
    "#         r.raise_for_status()\n",
    "#         c = r.content\n",
    "#         q = pyquery.PyQuery(c)\n",
    "#         q2 = q(\"#projects-index > [class='projects root']\")\n",
    "#         projects = _Project(self, \"All\", \"\")\n",
    "#         for e in q2.children(\".root\"):\n",
    "#             proj_ele = pyquery.PyQuery(e).children(\".root > a\")[0]\n",
    "#             pt = proj_ele.text\n",
    "#             phref = proj_ele.attrib['href'].split(\"/\")[-1]\n",
    "#             proj = projects.add(pt, phref)\n",
    "#             q4 = pyquery.PyQuery(e).children(\"[class='more collapsed']\")\n",
    "#             if len(q4) and _sp_re.match(q4[0].text):\n",
    "#                 q3 = pyquery.PyQuery(e)(\"[class='projects ']\")\n",
    "#                 for e2 in q3(\".child > .child > a\"):\n",
    "#                     proj.add(e2.text, e2.attrib['href'].split(\"/\")[-1])\n",
    "#         return projects\n",
    "\n",
    "    def download_projects(self):\n",
    "        url = uj(self._base_url, self._proj_url + \".xml\")\n",
    "        print(\"Downloading projects...\")\n",
    "        r = self._sess.get(url, auth=self._auth)\n",
    "        r.raise_for_status()\n",
    "        xml = lxml.etree.XML(r.content)\n",
    "        projects = {}\n",
    "        for proj in xml.findall(\"project\"):\n",
    "            p = Project.from_element(self, proj)\n",
    "            projects[p.name] = p\n",
    "        \n",
    "        # Second pass, process project subtasks\n",
    "        for p in projects.values():\n",
    "            if p.parent is not None:\n",
    "                parent = projects[p.parent['name']]\n",
    "                parent.add_subproject(p)\n",
    "        return projects\n",
    "    \n",
    "    def _download_gantt_raw(self, project):\n",
    "        url = (self._proj_issues_url % project) + \"/gantt\"\n",
    "        url = uj(self._base_url, url)\n",
    "        r1 = self._sess.get(url)\n",
    "        r1.raise_for_status()\n",
    "        return r1.content\n",
    "\n",
    "    def download_gantt(self, project):\n",
    "        project = self.projects(project).identifier\n",
    "        c = _download_gantt_raw(self, project)\n",
    "        q = pyquery.PyQuery(c)\n",
    "        q2 = q(\".gantt_subjects\")\n",
    "        i_list = []\n",
    "        for el in q2.children(\".issue-subject\"):\n",
    "            title = el.attrib['title']\n",
    "            e2=pyquery.PyQuery(el).children(\"span > a\")[0]\n",
    "            tracker, id = _name2id_re.match(e2.text).groups()\n",
    "            id = int(id)\n",
    "            i_list.append(id)\n",
    "        \n",
    "        project_issues = self.download_issues(project)\n",
    "        rv = [] \n",
    "        for i in i_list:\n",
    "            rv.append(project_issues[i])\n",
    "        \n",
    "        # sanity check list of subissues. There should be no cycles in this graph. \n",
    "        # Also this seems to run in a few ms, so no big deal.\n",
    "        # _map_issues(rv)\n",
    "        \n",
    "        return rv\n",
    "    \n",
    "    def _download_project_issues_iter(self, ops, limit, offset):\n",
    "        ops['limit'] = limit\n",
    "        ops['offset'] = offset\n",
    "        url = uj(self._base_url, self._issues_url + \".json\")\n",
    "        url += \"?\" + urllib.parse.urlencode(ops)\n",
    "        r = self._sess.get(url, auth=self._auth)\n",
    "        r.raise_for_status()\n",
    "        return r\n",
    "\n",
    "    def download_issues(self, project_id=None, created_on=None, modified_on=None):\n",
    "        ops = {}\n",
    "        if project_id:\n",
    "            if isinstance(project_id, str):\n",
    "                project_id = self.projects(project_id).id\n",
    "            ops['project_id'] = project_id\n",
    "            \n",
    "        # Unfortunately the api for querying dates and ranges is \n",
    "        # quite awkward to translate into a sensible python api\n",
    "        \n",
    "        if created_on:\n",
    "            if not isinstance(created_on, str):\n",
    "                raise TypeError(\"Argument created_on must be type str- try .isoformat() (got type %r)\" % created_on)\n",
    "            ops['created_on'] = created_on\n",
    "            \n",
    "        if modified_on:\n",
    "            if not isinstance(modified_on, str):\n",
    "                raise TypeError(\"Argument created_on must be type str- try .isoformat() (got type %r)\" % modified_on)\n",
    "            ops['modified_on'] = modified_on\n",
    "            \n",
    "        issues = {i.id: i for i in self._download_issues(ops)}\n",
    "        for iss in issues.values():\n",
    "            if iss.parent is not None:\n",
    "                id = iss.parent\n",
    "                iss.parent = issues[id]\n",
    "                issues[id].subtasks.append(iss)\n",
    "        return issues           \n",
    "            \n",
    "\n",
    "    def _download_issues(self, ops):\n",
    "        offset = 0\n",
    "        limit = 100\n",
    "        limit = min(max(limit, 0), 100)\n",
    "        total_count = 0\n",
    "        \n",
    "        \n",
    "        print(\"\\rDownloading issues...\", end=\"\")\n",
    "        while True:\n",
    "            r = self._download_project_issues_iter(ops, limit, offset)\n",
    "            d = json.loads(r.content.decode())\n",
    "            issues = d['issues']\n",
    "\n",
    "            if not issues:\n",
    "                break\n",
    "\n",
    "            yield from self._parse_issues(issues)\n",
    "\n",
    "            total_count = int(d.get('total_count',0))\n",
    "            offset += len(issues)\n",
    "            print(\"\\rDownloading issues: %d/%d      \" % (offset, total_count), end=\"\")\n",
    "            if offset >= total_count:\n",
    "                break\n",
    "        print()\n",
    "\n",
    "    def _parse_issues(self, issues):\n",
    "        for i in issues:\n",
    "            yield Issue.from_json(self, **i)\n",
    "    \n",
    "\n",
    "def _gantt_duplicate(issue, level):\n",
    "    raise ValueError(\"Found duplicate issue at level %d: %r\" % (level, issue.subject))\n",
    "\n",
    "def _map_issues_recursive(issues, seen, level, set_issues):\n",
    "    for i in issues:\n",
    "        if i.parent and i.parent in set_issues:\n",
    "            continue\n",
    "        if i in seen:\n",
    "            if level == 0:\n",
    "                continue\n",
    "            _gantt_duplicate(issue, level)\n",
    "        seen.add(i)\n",
    "        subt = i.subtasks\n",
    "        _map_issues_recursive(subt, seen, level+1)\n",
    "    return seen\n",
    "\n",
    "def _map_issues(issues):\n",
    "    try:\n",
    "        seen = _map_issues_recursive(issues, set(), 0, set(issues))\n",
    "    except ValueError as e:\n",
    "        e2 = ValueError()\n",
    "        e2.args = e.args\n",
    "        raise e2 from None\n",
    "    if len(seen) != len(issues):\n",
    "        raise ValueError(\"Internal error checking gantt integrity: len(seen) != len(issues).\")\n",
    "    if (set(issues) - seen):\n",
    "        raise ValueError(\"Internal error checking gantt integrity: Not all issues seen.\")\n",
    "\n",
    "    \n",
    "def _parse_custom_fields(e):\n",
    "    rv = {}\n",
    "    for cf in e.findall(\"custom_field\"):\n",
    "        cfd = {}\n",
    "        cfd.update(cf.attrib)\n",
    "        v = cf.find(\"value\")\n",
    "        if v is None or v.text == 'blank':\n",
    "            val = None\n",
    "        else:\n",
    "            val = v.text\n",
    "        cfd['value'] = val\n",
    "        rv[cfd['name']] = cfd\n",
    "    return rv\n",
    "\n",
    "def _parse_datetime(e):\n",
    "    return dateutil.parser.parse(e.text)\n",
    "\n",
    "def _parse_int(e):\n",
    "    return int(e.text)\n",
    "\n",
    "def _parse_bool(e):\n",
    "    t = e.text.lower()\n",
    "    if t == 'false':\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def _parse_parent(e):\n",
    "    return {k:v for k,v in e.attrib.items()}\n",
    "\n",
    "\n",
    "class Project():\n",
    "    def __init__(self, api, id=0, name=\"\", identifier=\"\", description=\"\", parent=None, status=None, \n",
    "                 is_public=False, custom_fields=None, created_on=None, updated_on=None):\n",
    "        self._api = api\n",
    "        self.name = name\n",
    "        self.id = id\n",
    "        self.name = name\n",
    "        self.identifier = identifier\n",
    "        self.description = description\n",
    "        self.parent = parent\n",
    "        self.status = status\n",
    "        self.is_public = is_public\n",
    "        self.custom_fields = custom_fields\n",
    "        self.created_on = created_on\n",
    "        self.updated_on = updated_on\n",
    "        self._subprojects = []\n",
    "        \n",
    "        \n",
    "    def add_subproject(self, sp):\n",
    "        sp.parent = self\n",
    "        if sp not in self._subprojects:\n",
    "            self._subprojects.append(sp)\n",
    "            \n",
    "    def __repr__(self):\n",
    "        return \"_Project(%s)\" % ', '.join(\"%s=%r\" % (k[0], getattr(self, k[0])) for k in self._proj_parse_table)\n",
    "    \n",
    "    def download_issues(self, utf8=True, columns='all'):\n",
    "        return self._api.download_project_issues(self.identifier, utf8, columns)\n",
    "    \n",
    "    def download_gantt(self):\n",
    "        return self._api.download_gantt(self.identifier)\n",
    "        \n",
    "    _proj_parse_table = [\n",
    "        # e.tag attr parse function\n",
    "        (\"id\", \"id\", _parse_int),\n",
    "        (\"name\", \"name\", None),\n",
    "        (\"identifier\", \"identifier\", None),\n",
    "        (\"description\", \"description\", None),\n",
    "        (\"parent\", \"parent\", _parse_parent),\n",
    "        (\"status\", \"status\", _parse_int),\n",
    "        (\"is_public\", \"is_public\", _parse_bool),\n",
    "        (\"custom_fields\", \"custom_fields\", _parse_custom_fields),\n",
    "        (\"created_on\", \"created_on\", _parse_datetime),\n",
    "        (\"updated_on\", \"updated_on\", _parse_datetime),\n",
    "    ]\n",
    "        \n",
    "    @classmethod\n",
    "    def from_element(cls, api, e):\n",
    "        kw = {}\n",
    "        for tag, k, func in cls._proj_parse_table:\n",
    "            el = e.find(tag)\n",
    "            if el is None:\n",
    "                continue\n",
    "            if func:\n",
    "                v = func(el)\n",
    "            else:\n",
    "                v = el.text\n",
    "            if v is not None:\n",
    "                kw[k] = v\n",
    "        if not kw and e.tag != 'project':\n",
    "            raise ValueError(\"Failed to parse element: element should be <project> element.\")\n",
    "        return cls(api, **kw)\n",
    "        \n",
    "def _unrecognized_kw(kw):\n",
    "    return ValueError(\"Unrecognized keywords: %s\" % (', '.join(repr(s) for s in kw)))\n",
    "\n",
    "def _iss_parse_datetime(api, a, v):\n",
    "    return dateutil.parser.parse(v)\n",
    "\n",
    "def _iss_parse_int(api, a, v):\n",
    "    return int(v)\n",
    "\n",
    "def _iss_parse_usr(api, a, v):\n",
    "    name = v.pop('name')\n",
    "    id = v.pop('id')\n",
    "    if v:\n",
    "        raise _unrecognized_kw(v)\n",
    "    return User(api, name, id)\n",
    "\n",
    "def _iss_parse_resource(api, a, v):\n",
    "    name = v.pop('name')\n",
    "    id = v.pop('id')\n",
    "    value = v.pop('value', \"\")\n",
    "    if v:\n",
    "        raise _unrecognized_kw(v)\n",
    "    return ResourceWithID(api, name, id, value)\n",
    "\n",
    "def _iss_parse_project(api, a, v):\n",
    "    return api.projects()[v['name']]\n",
    "\n",
    "def _iss_parse_parent(api, a, v):\n",
    "    if v:\n",
    "        return int(v['id'])\n",
    "\n",
    "def _parse_custom_fields(api, a, v):\n",
    "    fields = {}\n",
    "    for d in v:\n",
    "        name = d.pop('name')\n",
    "        id = d.pop('id')\n",
    "        val = d.pop('value', \"\")\n",
    "        if d:\n",
    "            raise _unrecognized_kw(d)\n",
    "        r = ResourceWithID(api, name, id, val)\n",
    "        fields[name] = val\n",
    "    return fields\n",
    "\n",
    "\n",
    "class ResourceWithID():\n",
    "    def __init__(self, api, name, id, value=\"\"):\n",
    "        self.api = api\n",
    "        self.name = name\n",
    "        self.id = id\n",
    "        self.value = value\n",
    "        \n",
    "    def __repr__(self):\n",
    "        n = self.__class__.__name__\n",
    "        args = ', '.join(\"%s=%r\" % (a, getattr(self, a)) for a in (\"name\", 'id'))\n",
    "        return \"%s(%s)\" % (n, args)\n",
    "\n",
    "\n",
    "class User(ResourceWithID):\n",
    "    def __init__(self, api, name, id):\n",
    "        super().__init__(api, name, id)\n",
    "        del self.value\n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "class Issue():\n",
    "    \n",
    "    _issue_parse_tbl = [\n",
    "        (\"author\", \"author\", _iss_parse_usr),\n",
    "        (\"custom_fields\", \"custom_fields\", _parse_custom_fields),\n",
    "        (\"fixed_version\", \"sprint_milestone\", _iss_parse_resource),  # oddly named. TODO double check this\n",
    "        (\"category\", \"category\", None),\n",
    "        (\"status\", \"status\", _iss_parse_resource),\n",
    "        (\"company\", \"company\", None),\n",
    "        (\"created_on\", \"created_on\", _iss_parse_datetime),\n",
    "        (\"description\", \"description\", None),\n",
    "        (\"subject\", \"subject\", None),\n",
    "        (\"done_ratio\", \"done_ratio\", None),\n",
    "        (\"crm_reply_token\", \"crm_reply_token\", None),\n",
    "        (\"updated_on\", \"updated_on\", _iss_parse_datetime),\n",
    "        (\"id\", \"id\", _iss_parse_int),\n",
    "        (\"project\", \"project\", _iss_parse_project),\n",
    "        (\"contact\", \"contact\", None),\n",
    "        (\"priority\", \"priority\", _iss_parse_resource),\n",
    "        (\"due_date\", \"due_date\", _iss_parse_datetime),\n",
    "        (\"estimated_hours\", \"estimated_hours\", None),\n",
    "        (\"tracker\", \"tracker\", _iss_parse_resource),\n",
    "        (\"parent\", \"parent\", _iss_parse_parent),\n",
    "        (\"closed_on\", \"closed_on\", _iss_parse_datetime),\n",
    "        (\"start_date\", \"start_date\", _iss_parse_datetime),\n",
    "        (\"tracking_uri\", \"tracking_uri\", None),\n",
    "        (\"assigned_to\", \"assigned_to\", _iss_parse_usr)\n",
    "    ]\n",
    "\n",
    "    def __init__(self, api, author=None, custom_fields=None, sprint_milestone=None, category=None, status=None, \n",
    "                  company=None, created_on=None, description=None, subject=None, done_ratio=None, crm_reply_token=None, \n",
    "                  updated_on=None, id=None, project=None, contact=None, priority=None, due_date=None, estimated_hours=None, \n",
    "                  tracker=None, parent=None, closed_on=None, start_date=None, tracking_uri=None, assigned_to=None):\n",
    "        \n",
    "        self._api = api\n",
    "        \n",
    "        self.author = author\n",
    "        self.custom_fields = custom_fields\n",
    "        self.sprint_milestone = sprint_milestone  \n",
    "        self.category = category\n",
    "        self.status = status\n",
    "        self.company = company\n",
    "        self.created_on = created_on\n",
    "        self.description = description\n",
    "        self.subject = subject\n",
    "        self.done_ratio = done_ratio\n",
    "        self.crm_reply_token = crm_reply_token\n",
    "        self.updated_on = updated_on\n",
    "        self.id = id\n",
    "        self.project = project\n",
    "        self.contact = contact\n",
    "        self.priority = priority\n",
    "        self.due_date = due_date\n",
    "        self.estimated_hours = estimated_hours\n",
    "        self.tracker = tracker\n",
    "        self.parent = parent\n",
    "        self.closed_on = closed_on\n",
    "        self.start_date = start_date\n",
    "        self.tracking_uri = tracking_uri\n",
    "        self.assigned_to = assigned_to\n",
    "        \n",
    "        self.subtasks = []\n",
    "        \n",
    "    def __eq__(self, other):\n",
    "        if not isinstance(other, self.__class__):\n",
    "            return NotImplemented\n",
    "        na1 = object()\n",
    "        na2 = object()\n",
    "        for _, attr, _ in self._issue_parse_tbl:\n",
    "            v1 = getattr(self, attr, na1)\n",
    "            v2 = getattr(other, attr, na2)\n",
    "            if v1 != v2:\n",
    "                return False\n",
    "        return True\n",
    "                \n",
    "\n",
    "    @classmethod\n",
    "    def from_json(cls, api, **kw):\n",
    "        dct = {}\n",
    "        absent = object()\n",
    "        for k, attr, func in cls._issue_parse_tbl:\n",
    "            v = kw.pop(k, absent)\n",
    "            if v is absent or not v:\n",
    "                continue\n",
    "            if func:\n",
    "                v = func(api, attr, v)\n",
    "            dct[attr] = v\n",
    "        if kw:\n",
    "            raise _unrecognized_kw(kw)\n",
    "        return cls(api, **dct)\n",
    "    \n",
    "    def add_subtask(self, issue):\n",
    "        self.subtasks.append(issue)\n",
    "\n",
    "    def pretty_print(self):\n",
    "        \n",
    "        def reprify(v):\n",
    "            if isinstance(v, self.__class__):\n",
    "                return \"%s(...)\" % self.__class__.__name__\n",
    "            if isinstance(v, dict):\n",
    "                return \"{...}\"\n",
    "            if isinstance(v, list):\n",
    "                return \"[...]\"\n",
    "            return repr(v)\n",
    "        \n",
    "        attrs = [t[1] for t in self._issue_parse_tbl]\n",
    "        args = \", \".join(\"%s=%s\" % (a, reprify(getattr(self, a))) for a in attrs)\n",
    "        if not args: args = '<empty>'\n",
    "        return \"Issue(%s)\" % args\n",
    "\n",
    "    def download(self, type='pdf'):\n",
    "        return self._api.download_issue_pdf(self.id)\n",
    "    \n",
    "    __str__ = __repr__ = pretty_print\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 483,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test_filter(i):\n",
    "    d=i.download_issues('pbssoftware', \">=2016-08-10\")\n",
    "    def iter5(ob):\n",
    "        rv=[];ap=rv.append\n",
    "        for _ in range(5):\n",
    "            ap(next(ob,None))\n",
    "        return rv\n",
    "    import collections; exhaust = collections.deque(maxlen=0).extend\n",
    "    l = list(d)\n",
    "    import datetime\n",
    "    d2 = datetime.datetime(2016, 8, 10).date()\n",
    "    for item in l:\n",
    "        co = item.created_on.date()\n",
    "        mark = \"<\" if co < d2 else \">\" if co > d2 else \"=\"\n",
    "        print(\"Issue #%d\" % item.id, item.created_on, mark, d2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 481,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing files to disk: 109/109         \n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "import threading\n",
    "import queue\n",
    "import time\n",
    "import os\n",
    "\n",
    "def download_worker(api, in_q, out_q, path):\n",
    "    api.login()\n",
    "    while True:\n",
    "        iss = in_q.get(True)\n",
    "        if iss is None:\n",
    "            return\n",
    "        pdf = api.download_issue_pdf(iss.id)\n",
    "        fn = \"%s/%d.pdf\" % (path, iss.id)\n",
    "        out_q.put((fn, pdf))\n",
    "        \n",
    "def write(path, raw):\n",
    "    with open(path, 'wb') as f:\n",
    "        f.write(raw)\n",
    "        \n",
    "        \n",
    "def write_queue_to_disk(out_q, print_update2):\n",
    "    # write to disk\n",
    "    while True:\n",
    "        try:\n",
    "            fn, pdf = out_q.get(False)\n",
    "        except queue.Empty:\n",
    "            break\n",
    "        else:\n",
    "            write(fn, pdf)\n",
    "        print_update2()\n",
    "        \n",
    "        \n",
    "def download_issues_pdf_multithread(api, issues, path=\".\", filter_cb=lambda i: True, max_threads=8):\n",
    "    \n",
    "    os.makedirs(path, exist_ok=True)\n",
    "    issues = [i for i in issues if filter_cb(i)]\n",
    "\n",
    "    in_q = queue.Queue()\n",
    "    out_q = queue.Queue()\n",
    "    threads = set()\n",
    "    \n",
    "    def print_update():\n",
    "        nonlocal issues, in_q, max_threads\n",
    "        nissues = len(issues)\n",
    "        qsz = in_q.qsize()\n",
    "        done = nissues - qsz\n",
    "        if done < 0:\n",
    "            done = 0  # compensate for sentinels inserted into queue\n",
    "        print(\"\\rDownloading files with %d threads multithreaded %d/%d           \" % (max_threads, done, nissues), end=\"\")\n",
    "        \n",
    "    def print_update2():\n",
    "        nonlocal out_q, issues\n",
    "        done = len(issues) - out_q.qsize()\n",
    "        print(\"\\rWriting files to disk: %d/%d         \" % (done, len(issues)), end=\"\")\n",
    "    \n",
    "    for i in range(max_threads):\n",
    "        t = threading.Thread(None, download_worker, args=(api.copy(), in_q, out_q, path), daemon=True)\n",
    "        threads.add(t)\n",
    "    for t in threads:\n",
    "        t.start()\n",
    "    for i in issues:\n",
    "        in_q.put(i)\n",
    "    for i in range(max_threads):\n",
    "        in_q.put(None)\n",
    "        \n",
    "    # download stuff\n",
    "    while True:\n",
    "        alive = False\n",
    "        tcopy = list(threads)\n",
    "\n",
    "        for t in tcopy:\n",
    "            if not t.is_alive():\n",
    "                threads.remove(t)\n",
    "\n",
    "        print_update()\n",
    "        time.sleep(.5)\n",
    "        if not threads:\n",
    "              break\n",
    "        write_queue_to_disk(out_q, print_update2)\n",
    "    write_queue_to_disk(out_q, print_update2)\n",
    "    print()\n",
    "    print(\"Done\")\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 619,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [200]>"
      ]
     },
     "execution_count": 619,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "api = IssuetrackerAPI('issue.pbsbiotech.com', 'nstarkweather', 'kookychemist')\n",
    "api.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 621,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading projects...\n",
      "Downloading issues: 671/671      \n"
     ]
    }
   ],
   "source": [
    "issues= api.download_issues('pbssoftware', modified_on=\">=2016-6-01\")\n",
    "issues = list(issues.values())\n",
    "\n",
    "def filter_cb(iss):\n",
    "    return iss.sprint_milestone.name == \"3.0\" and iss.tracker.name == \"Specification\"\n",
    "\n",
    "#path = \"pdfs3\"\n",
    "#download_issues_pdf_multithread(i, issues, path, filter_cb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 623,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading issues: 671/671      \n"
     ]
    }
   ],
   "source": [
    "gantt_issues = api.download_gantt('pbssoftware')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 591,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def iter2(subtasks, level):\n",
    "    for i in subtasks or ():\n",
    "        yield level+1, i\n",
    "        for task in iter2(i.subtasks, level+1):\n",
    "            yield task\n",
    "\n",
    "def iter_subtasks1(issues):\n",
    "    issues = [i for i in issues if i.parent is None]\n",
    "    for i in issues:\n",
    "        yield 0, i\n",
    "        for lvl, task in iter2(i.subtasks, 1) or ():\n",
    "            yield lvl, task\n",
    "            \n",
    "def iter_subtasks2(issues, seen=None, level=0):\n",
    "    if seen is None:\n",
    "        seen = set()\n",
    "    for i in issues:\n",
    "        if level == 0 and i.parent is not None:\n",
    "            continue\n",
    "        if i in seen:\n",
    "            continue\n",
    "        seen.add(i)\n",
    "        yield level, i\n",
    "        yield from iter_subtasks2(i.subtasks, seen, level+1)\n",
    "    if level == 0:\n",
    "        print(len(seen), len(issues))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 659,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "495 495\n"
     ]
    }
   ],
   "source": [
    "maxlen = max(len(i.subject) for i in gantt_issues)\n",
    "fmt = \"%.20s\"\n",
    "ss = []\n",
    "issues2 = {i.id for i in issues if i.sprint_milestone.name == \"3.0\" and i.tracker.name == 'Specification'}\n",
    "gi = list(iter_subtasks2(gantt_issues))\n",
    "for lvl, i in gi:\n",
    "    if i.id not in issues2:\n",
    "        continue\n",
    "    link = api._base_url + \"/\" + \"issues/\" + str(i.id)\n",
    "    s=\"%d,%s,%s\\n\" % (lvl, i.subject, link)\n",
    "    ss.append(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 566,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def _gantt_duplicate(issue, level):\n",
    "    raise ValueError(\"Found duplicate issue at level %d: %r\" % (level, issue.subject))\n",
    "\n",
    "def _map_issues_recursive(issues, seen, level):\n",
    "    for i in issues:\n",
    "        if i in seen:\n",
    "            if level == 0:\n",
    "                continue\n",
    "            _gantt_duplicate(issue, level)\n",
    "        seen.add(i)\n",
    "        subt = i.subtasks\n",
    "        _map_issues_recursive(subt, seen, level+1)\n",
    "    return seen\n",
    "\n",
    "def _map_issues(issues):\n",
    "    try:\n",
    "        seen = _map_issues_recursive(issues, set(), 0)\n",
    "    except ValueError as e:\n",
    "        e2 = ValueError()\n",
    "        e2.args = e.args\n",
    "        raise e2 from None\n",
    "    if len(seen) != len(issues):\n",
    "        raise ValueError(\"Internal error checking gantt integrity: len(seen) != len(issues).\")\n",
    "    if (set(issues) - seen):\n",
    "        raise ValueError(\"Internal error checking gantt integrity: Not all issues seen.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 665,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(\"pdfs3/issue_outline.txt\", 'wb') as f:\n",
    "    f.write(''.join(ss).encode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 633,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResourceWithID(name='Future Release', id=33)"
      ]
     },
     "execution_count": 633,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i.sprint_milestone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 654,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Issue(author=User(name='James Small', id=42), custom_fields={...}, sprint_milestone=ResourceWithID(name='3.0', id=52), category=None, status=ResourceWithID(name='Unit Test', id=10), company=None, created_on=datetime.datetime(2015, 4, 15, 18, 52, 48, tzinfo=tzutc()), description='*FRS2304.0*\\r\\nFor the web calls and shell actions affected by the permission table, check the user permission before executing the action. If the user does not have permission, do not execute.\\r\\n\\r\\nNotes:\\r\\nThe attached Excel spreadsheet specifies all LabVIEW UI functions, and Server calls, along with their associated permission option.  ', subject='Granular permissions', done_ratio=None, crm_reply_token=None, updated_on=datetime.datetime(2016, 2, 17, 22, 46, 50, tzinfo=tzutc()), id=2304, project=_Project(id=5, name='Software', identifier='pbssoftware', description='This project collects all issues and features in regards to software', parent=None, status=1, is_public=False, custom_fields={'Customer Information': {'name': 'Customer Information', 'id': '2', 'value': None}}, created_on=datetime.datetime(2010, 10, 14, 10, 55, 55, tzinfo=tzutc()), updated_on=datetime.datetime(2010, 10, 14, 11, 7, 58, tzinfo=tzutc())), contact=None, priority=ResourceWithID(name='Normal', id=4), due_date=None, estimated_hours=8.0, tracker=ResourceWithID(name='Specification', id=13), parent=Issue(...), closed_on=None, start_date=datetime.datetime(2015, 4, 15, 0, 0), tracking_uri=None, assigned_to=User(name='James Small', id=42))"
      ]
     },
     "execution_count": 654,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(issues2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 664,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1,Login Behavior,https://issue.pbsbiotech.com/issues/989\\n',\n",
       " '1,Webcontrol.conf file ,https://issue.pbsbiotech.com/issues/1885\\n',\n",
       " '1,Recipe Steps in Reports are out of order,https://issue.pbsbiotech.com/issues/1262\\n',\n",
       " '1,Opening Reports in WebUI in Kiosk Mode (Christian placeholder),https://issue.pbsbiotech.com/issues/2309\\n',\n",
       " '1,File Checksum,https://issue.pbsbiotech.com/issues/2316\\n',\n",
       " '1,Errors report csv has entries in wrong columns,https://issue.pbsbiotech.com/issues/1263\\n',\n",
       " '1,Database sometimes gets corrupt,https://issue.pbsbiotech.com/issues/420\\n',\n",
       " '1,Alarms triggered on RIO power cycle,https://issue.pbsbiotech.com/issues/2522\\n',\n",
       " '1,Report Generation Bug,https://issue.pbsbiotech.com/issues/2596\\n',\n",
       " '1,Web Login Improvements (Christian placeholder),https://issue.pbsbiotech.com/issues/2715\\n',\n",
       " '2,Web UI Login events show \"Unknown User\",https://issue.pbsbiotech.com/issues/2115\\n',\n",
       " '2,Web UI Logout unknown user,https://issue.pbsbiotech.com/issues/2445\\n',\n",
       " '2,End Batch should be last user event recorded for batch,https://issue.pbsbiotech.com/issues/2458\\n',\n",
       " '2,Changes to Existing User Events,https://issue.pbsbiotech.com/issues/1073\\n',\n",
       " '2,Changes to Load Bag behavior,https://issue.pbsbiotech.com/issues/2906\\n',\n",
       " '2,Changes to Clearing Alarms behavior,https://issue.pbsbiotech.com/issues/2908\\n',\n",
       " '1,Auto DB Backup,https://issue.pbsbiotech.com/issues/2734\\n',\n",
       " '1,Generate random junk for license.bin creation,https://issue.pbsbiotech.com/issues/2757\\n',\n",
       " '1,Change password for auth sb RIO,https://issue.pbsbiotech.com/issues/2758\\n',\n",
       " '1,Auto archive DB and create new one,https://issue.pbsbiotech.com/issues/1759\\n',\n",
       " '1,Lock down Chrome,https://issue.pbsbiotech.com/issues/2808\\n',\n",
       " '1,Lock down BIOS,https://issue.pbsbiotech.com/issues/2814\\n',\n",
       " '1,Change Process Data records to be in RIO time,https://issue.pbsbiotech.com/issues/2825\\n',\n",
       " '1,Lock down Windows (tasks),https://issue.pbsbiotech.com/issues/2828\\n',\n",
       " '1,Change database name,https://issue.pbsbiotech.com/issues/2840\\n',\n",
       " '1,Report Generation,https://issue.pbsbiotech.com/issues/2851\\n',\n",
       " '1,Version Numbering and random VI changes,https://issue.pbsbiotech.com/issues/2855\\n',\n",
       " '1,Make sure RIO time is correct,https://issue.pbsbiotech.com/issues/1845\\n',\n",
       " '1,Database History file,https://issue.pbsbiotech.com/issues/2875\\n',\n",
       " '1,Cfg file initial states,https://issue.pbsbiotech.com/issues/2890\\n',\n",
       " '1,Access DB character limit,https://issue.pbsbiotech.com/issues/2895\\n',\n",
       " '1,Specify event timestamp externally from sub VIs,https://issue.pbsbiotech.com/issues/2905\\n',\n",
       " '1,Bioreactor Name storage,https://issue.pbsbiotech.com/issues/2907\\n',\n",
       " '1,RIO write to persistent memory if Atom unavailable,https://issue.pbsbiotech.com/issues/2997\\n',\n",
       " '1,Verify Action Before Writing to Database,https://issue.pbsbiotech.com/issues/2998\\n',\n",
       " '2,File Dialog and Browser,https://issue.pbsbiotech.com/issues/2819\\n',\n",
       " '2,Shell Interface for 3.0,https://issue.pbsbiotech.com/issues/2856\\n',\n",
       " '2,Graph Panel,https://issue.pbsbiotech.com/issues/2859\\n',\n",
       " '2,Welcome page,https://issue.pbsbiotech.com/issues/2884\\n',\n",
       " '2,Login Screen,https://issue.pbsbiotech.com/issues/2885\\n',\n",
       " '2,System Tools menu,https://issue.pbsbiotech.com/issues/2886\\n',\n",
       " '2,About menu,https://issue.pbsbiotech.com/issues/2891\\n',\n",
       " '2,Power VI,https://issue.pbsbiotech.com/issues/2922\\n',\n",
       " '2,New Session Framework,https://issue.pbsbiotech.com/issues/2367\\n',\n",
       " \"2,Don't allow 2 or more 'Start Batch' or 'End Batch' calls in a row,https://issue.pbsbiotech.com/issues/2888\\n\",\n",
       " '2,getUserInfo,https://issue.pbsbiotech.com/issues/2894\\n',\n",
       " '2,getUsers Server Call contents,https://issue.pbsbiotech.com/issues/2897\\n',\n",
       " '2,User event for revert trial cal call,https://issue.pbsbiotech.com/issues/2910\\n',\n",
       " '2,JSON calls,https://issue.pbsbiotech.com/issues/2918\\n',\n",
       " '2,Shutdown Server Call,https://issue.pbsbiotech.com/issues/2924\\n',\n",
       " \"2,'Show Desktop' call,https://issue.pbsbiotech.com/issues/2975\\n\",\n",
       " '1,Reject m or b if Inf or -Inf,https://issue.pbsbiotech.com/issues/1902\\n',\n",
       " '1,Granular permissions,https://issue.pbsbiotech.com/issues/2304\\n',\n",
       " '1,Alarm and Logger settings config files need errors,https://issue.pbsbiotech.com/issues/977\\n',\n",
       " '1,Pressure PV color (Christian placeholder),https://issue.pbsbiotech.com/issues/2394\\n',\n",
       " '1,Base Pump selector,https://issue.pbsbiotech.com/issues/2395\\n',\n",
       " '1,Any recipe with \"-\" in name is looped,https://issue.pbsbiotech.com/issues/2404\\n',\n",
       " '1,Remove unnecessary System Variables,https://issue.pbsbiotech.com/issues/2415\\n',\n",
       " '1,Startup alarms not audible,https://issue.pbsbiotech.com/issues/2460\\n',\n",
       " '1,Report Email Improvements,https://issue.pbsbiotech.com/issues/2476\\n',\n",
       " '1,Improved Select Sensors behavior,https://issue.pbsbiotech.com/issues/1583\\n',\n",
       " '1,Recipes and globals in clusters,https://issue.pbsbiotech.com/issues/2769\\n',\n",
       " '1,MFC slope and intercept in cal factors file,https://issue.pbsbiotech.com/issues/1754\\n',\n",
       " '1,getDORAValues Error with RIO disconnected,https://issue.pbsbiotech.com/issues/2780\\n',\n",
       " '1,Correct RIO SN reporting,https://issue.pbsbiotech.com/issues/2787\\n',\n",
       " '1,Allow all alarms to snooze,https://issue.pbsbiotech.com/issues/2796\\n',\n",
       " '1,Web UI pH and DO cals from raw values,https://issue.pbsbiotech.com/issues/2809\\n',\n",
       " '1,Correct CO2 and N2% Calculation,https://issue.pbsbiotech.com/issues/2877\\n',\n",
       " \"1,Can't log in to Web UI if DB is too big,https://issue.pbsbiotech.com/issues/2035\\n\",\n",
       " '1,Individual fuses, and comb plate, should have alarms,https://issue.pbsbiotech.com/issues/2210\\n',\n",
       " '1,Web UI Main Gas button and Graphs tab changes needed (Christian placeholder),https://issue.pbsbiotech.com/issues/2231\\n',\n",
       " '1,Clean up getMainValues call,https://issue.pbsbiotech.com/issues/2320\\n',\n",
       " '1,Merge getMainInfo and getVersion calls into getStatic,https://issue.pbsbiotech.com/issues/2321\\n',\n",
       " '1,Air and Mag System Variables should have same Max Temp name,https://issue.pbsbiotech.com/issues/2516\\n',\n",
       " '1,Remove modifications to current System Variables.cfg file in getConfig Server Call response,https://issue.pbsbiotech.com/issues/2805\\n',\n",
       " '1,Web UI Advanced View changes (Christian placeholder),https://issue.pbsbiotech.com/issues/1783\\n',\n",
       " '0,Defer Panel Updates for Graph Panel,https://issue.pbsbiotech.com/issues/2502\\n',\n",
       " '0,Re-name DO 1 point Cal button \"Span\",https://issue.pbsbiotech.com/issues/2626\\n',\n",
       " '0,Globals to remove,https://issue.pbsbiotech.com/issues/2669\\n',\n",
       " '0,Service UI keyboard and numpad,https://issue.pbsbiotech.com/issues/2703\\n',\n",
       " '1,Recipe Editor Modifications,https://issue.pbsbiotech.com/issues/2860\\n',\n",
       " '1,Changes to Logger Settings editor,https://issue.pbsbiotech.com/issues/2864\\n',\n",
       " '1,Changes to Alarm editor,https://issue.pbsbiotech.com/issues/2865\\n',\n",
       " '1,Cal Sensors Modifications,https://issue.pbsbiotech.com/issues/2880\\n',\n",
       " '1,User Control Panel and PBSUsers.conf file Modifications,https://issue.pbsbiotech.com/issues/2881\\n',\n",
       " '1,System Variables editor,https://issue.pbsbiotech.com/issues/2926\\n',\n",
       " '0,Modify DB Logger/DB Client for Handling Backlog,https://issue.pbsbiotech.com/issues/2871\\n',\n",
       " '0,Error Dialog Box,https://issue.pbsbiotech.com/issues/2876\\n',\n",
       " '0,Record globals only when user changes it through UI or recipe,https://issue.pbsbiotech.com/issues/3008\\n',\n",
       " '0,Map Network Drive in File Browser,https://issue.pbsbiotech.com/issues/3010\\n',\n",
       " '0,AuthsbRIO must delete license.bin file after completion,https://issue.pbsbiotech.com/issues/3011\\n',\n",
       " '0,Centralize Data recording,https://issue.pbsbiotech.com/issues/3012\\n']"
      ]
     },
     "execution_count": 664,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 658,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "for n, iss in gi:\n",
    "    print(i is iss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
